{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"3a39caac46f94a91b820594aecdfa6cc":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_1c9319480b9b43aa90ffa512014ea62d","IPY_MODEL_cfbf4bf823664a4f902f2cea0cc616f6"],"layout":"IPY_MODEL_f9247d172ae74aae9300f17abab71bc9"}},"1c9319480b9b43aa90ffa512014ea62d":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a1c5db9b8a748c785fae5f9613b48ad","placeholder":"​","style":"IPY_MODEL_411fdaa25a1e4342ac66220389a46058","value":"0.020 MB of 0.020 MB uploaded (0.000 MB deduped)\r"}},"cfbf4bf823664a4f902f2cea0cc616f6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_3cb8b83391f34cc5b7b3a462c770a86a","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bcfa414ffd64410f93f38d95b3dcb299","value":1}},"f9247d172ae74aae9300f17abab71bc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a1c5db9b8a748c785fae5f9613b48ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"411fdaa25a1e4342ac66220389a46058":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3cb8b83391f34cc5b7b3a462c770a86a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcfa414ffd64410f93f38d95b3dcb299":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Introduction\n","\n"],"metadata":{"id":"eYilWIGaFgCw"}},{"cell_type":"markdown","source":["This is an implementation of semantic segmentation transfer learning.\n","<br><br>\n","The model to be implemented here is based on the model developed in the paper \"*The devil is in the labels: Semantic segmentation from sentences*\".\n","<br>\n","The authors claim that their method, while operating in zero-shot setting, achieves results comparable to those of supervised learning; and it was possible by replacing the class labels with embeddings generated from sentences. Furthermore, fine-tuning the model to a given semantic segmentation dataset, should further improve the performance of the model.\n","<br>\n","The target is to adapt the original model to a new dataset; in this case, the *CMP Facade Database*\n","<br><br>\n","**Development Steps:**\n","*   Copy required functions from the original paper code to this Google Colab document (and update as required)\n","*   Create an iterator for the new dataset to suit the implementation\n","*   Allow the model to freeze the encoder while updating gradients for the head\n","*   Give meaningful explanation to the *CMP Facade Database* labels and use them to build embeddings. The explanations are obtained from [*merriam-webster*](https://www.merriam-webster.com/) while the embeddings are build with the  CLIP-ViT model (Same as the paper)\n","*    Integrate [*Weights & Biases*](https://wandb.ai/) to track experiment results. This step is done to show the relation between hyper-parameters and model performance\n","*    Fine-tune the model using the *CMP Facade Database*\n","*    Evaluate the performance of the model using the Mean IoU metric\n","<br>\n","\n","**Notes:**\n","*    The *CMP Facade Database* consists of two datasets, base ane extended. The base dataset is used for training while the extended extended dataset is used for testing\n","*    The GPU memory may sometimes be filled, that is why garbage collection is called on multiple points in the code\n","\n","**Links:**\n","*    [\"The devil is in the labels: Semantic segmentation from sentences\" Paper](https://arxiv.org/abs/2202.02002)\n","*    [Original Code Github Repository](https://github.com/irfanICMLL/SSIW/tree/master)\n","*    [CMP Facade Database](https://cmp.felk.cvut.cz/~tylecr1/facade/)\n","*    A link is provided in the Results section below for the results report"],"metadata":{"id":"BN2ls2HGHtjX"}},{"cell_type":"markdown","source":["# Implementation"],"metadata":{"id":"xUHwE3YXHw7Q"}},{"cell_type":"markdown","source":["Parameters that can be tweaked in the implementation:\n","\n","\n","```\n","USE_WANDB        #bool                 | whether to use W&B or not\n","MAX_MEMORY       #int                  | max image memory (used to avoid OutOfMemoryError)\n","DEFINITIONS_TYPE #\"word\" or \"sentence\" | label definition types\n","MODEL_NAME       #str                  | name used to save the fine-tuned model\n","OPTIMIZER_NAME   #\"adam\" or \"sgd\"      | optimizer to use\n","LR               #float                | learning rate\n","EPOCHS           #int                  | number of training epochs\n","```\n","\n"],"metadata":{"id":"5O0hepk30Cty"}},{"cell_type":"code","source":["#install packages required for original paper code\n","!pip install -U openmim  \n","!mim install mmcv-full \n","!pip install mmsegmentation\n","!pip install timm\n","!pip install tqdm\n","!pip install wandb\n","!pip install git+https://github.com/openai/CLIP.git"],"metadata":{"id":"yTN5zeJNAsrn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672392656318,"user_tz":-120,"elapsed":79987,"user":{"displayName":"Hussein Fawzy","userId":"10172350209806464456"}},"outputId":"4f0e3d62-2132-495a-dd2b-327f7e03768a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting openmim\n","  Downloading openmim-0.3.4-py2.py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 6.2 MB/s \n","\u001b[?25hCollecting colorama\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from openmim) (0.8.10)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from openmim) (1.3.5)\n","Collecting rich\n","  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n","\u001b[K     |████████████████████████████████| 237 kB 61.7 MB/s \n","\u001b[?25hCollecting model-index\n","  Downloading model_index-0.1.11-py3-none-any.whl (34 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from openmim) (2.23.0)\n","Requirement already satisfied: pip>=19.3 in /usr/local/lib/python3.8/dist-packages (from openmim) (21.1.3)\n","Requirement already satisfied: Click in /usr/local/lib/python3.8/dist-packages (from openmim) (7.1.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from model-index->openmim) (6.0)\n","Requirement already satisfied: markdown in /usr/local/lib/python3.8/dist-packages (from model-index->openmim) (3.4.1)\n","Collecting ordered-set\n","  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown->model-index->openmim) (5.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown->model-index->openmim) (3.11.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->openmim) (2022.6)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas->openmim) (1.21.6)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->openmim) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->openmim) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->openmim) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->openmim) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->openmim) (2022.12.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->openmim) (1.24.3)\n","Collecting commonmark<0.10.0,>=0.9.0\n","  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n","\u001b[K     |████████████████████████████████| 51 kB 8.6 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from rich->openmim) (4.4.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich->openmim) (2.6.1)\n","Installing collected packages: ordered-set, commonmark, rich, model-index, colorama, openmim\n","Successfully installed colorama-0.4.6 commonmark-0.9.1 model-index-0.1.11 openmim-0.3.4 ordered-set-4.1.0 rich-12.6.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.openmmlab.com/mmcv/dist/cu116/torch1.13.0/index.html\n","Collecting mmcv-full\n","  Downloading https://download.openmmlab.com/mmcv/dist/cu116/torch1.13.0/mmcv_full-1.7.1-cp38-cp38-manylinux1_x86_64.whl (46.1 MB)\n","\u001b[K     |████████████████████████████████| 46.1 MB 375 kB/s \n","\u001b[?25hCollecting yapf\n","  Downloading yapf-0.32.0-py2.py3-none-any.whl (190 kB)\n","\u001b[K     |████████████████████████████████| 190 kB 37.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from mmcv-full) (1.21.6)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from mmcv-full) (6.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from mmcv-full) (7.1.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from mmcv-full) (21.3)\n","Requirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.8/dist-packages (from mmcv-full) (4.6.0.66)\n","Collecting addict\n","  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->mmcv-full) (3.0.9)\n","Installing collected packages: yapf, addict, mmcv-full\n","Successfully installed addict-2.4.0 mmcv-full-1.7.1 yapf-0.32.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mmsegmentation\n","  Downloading mmsegmentation-0.29.1-py3-none-any.whl (828 kB)\n","\u001b[K     |████████████████████████████████| 828 kB 35.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from mmsegmentation) (1.21.6)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from mmsegmentation) (3.2.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from mmsegmentation) (21.3)\n","Collecting mmcls>=0.20.1\n","  Downloading mmcls-0.25.0-py2.py3-none-any.whl (648 kB)\n","\u001b[K     |████████████████████████████████| 648 kB 70.6 MB/s \n","\u001b[?25hRequirement already satisfied: prettytable in /usr/local/lib/python3.8/dist-packages (from mmsegmentation) (3.5.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mmsegmentation) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mmsegmentation) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mmsegmentation) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mmsegmentation) (1.4.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->mmsegmentation) (1.15.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prettytable->mmsegmentation) (0.2.5)\n","Installing collected packages: mmcls, mmsegmentation\n","Successfully installed mmcls-0.25.0 mmsegmentation-0.29.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting timm\n","  Downloading timm-0.6.12-py3-none-any.whl (549 kB)\n","\u001b[K     |████████████████████████████████| 549 kB 35.9 MB/s \n","\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from timm) (0.14.0+cu116)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from timm) (6.0)\n","Collecting huggingface-hub\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 76.9 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.8/dist-packages (from timm) (1.13.0+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm) (4.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (3.8.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (2.23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (4.64.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub->timm) (3.0.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (3.0.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->timm) (1.21.6)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->timm) (7.1.2)\n","Installing collected packages: huggingface-hub, timm\n","Successfully installed huggingface-hub-0.11.1 timm-0.6.12\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting wandb\n","  Downloading wandb-0.13.7-py2.py3-none-any.whl (1.9 MB)\n","\u001b[K     |████████████████████████████████| 1.9 MB 36.9 MB/s \n","\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (7.1.2)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.12.1-py2.py3-none-any.whl (174 kB)\n","\u001b[K     |████████████████████████████████| 174 kB 81.3 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n","Collecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Collecting setproctitle\n","  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n","Collecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)\n","\u001b[K     |████████████████████████████████| 184 kB 68.5 MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.23.0)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.3)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 1.7 MB/s \n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.12.0-py2.py3-none-any.whl (173 kB)\n","\u001b[K     |████████████████████████████████| 173 kB 19.4 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.11.1-py2.py3-none-any.whl (168 kB)\n","\u001b[K     |████████████████████████████████| 168 kB 57.5 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.11.0-py2.py3-none-any.whl (168 kB)\n","\u001b[K     |████████████████████████████████| 168 kB 62.3 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.10.1-py2.py3-none-any.whl (166 kB)\n","\u001b[K     |████████████████████████████████| 166 kB 67.0 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.10.0-py2.py3-none-any.whl (166 kB)\n","\u001b[K     |████████████████████████████████| 166 kB 60.2 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.10-py2.py3-none-any.whl (162 kB)\n","\u001b[K     |████████████████████████████████| 162 kB 59.2 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n","\u001b[K     |████████████████████████████████| 162 kB 38.3 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n","\u001b[K     |████████████████████████████████| 158 kB 17.3 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 67.6 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 82.4 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 84.7 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 78.9 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 80.7 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 80.4 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 85.9 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n","\u001b[K     |████████████████████████████████| 156 kB 83.6 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pathtools\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=54f4ec64d8a2be39b3ca2510b1c13e78b59e838d4a2cbcaaf8afb7e1ff9b9213\n","  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n","Successfully built pathtools\n","Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n","Successfully installed GitPython-3.1.30 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 wandb-0.13.7\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-94bge70h\n","  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-94bge70h\n","Collecting ftfy\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[K     |████████████████████████████████| 53 kB 1.3 MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (2022.6.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (4.64.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (1.13.0+cu116)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (0.14.0+cu116)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->clip==1.0) (0.2.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->clip==1.0) (4.4.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (2.23.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n","Building wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369408 sha256=cd8caf56ffae4d7c936ed1ce03dd2cf4e5de5838a4160e9d136e8f31617f87ff\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-mcz2wj12/wheels/ab/4f/3a/5e51521b55997aa6f0690e095c08824219753128ce8d9969a3\n","Successfully built clip\n","Installing collected packages: ftfy, clip\n","Successfully installed clip-1.0 ftfy-6.1.1\n"]}]},{"cell_type":"code","source":["#import libraries\n","import clip\n","import cv2\n","import gc\n","import math\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import requests\n","import torch\n","import torch.distributed as dist\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import wandb\n","\n","from abc import ABCMeta\n","from functools import partial\n","from mmcv.cnn import ConvModule\n","from mmcv.cnn.bricks import build_norm_layer\n","from mmcv.runner import load_checkpoint\n","from mmseg.ops import resize\n","from mmseg.utils import get_root_logger\n","from pathlib import Path\n","from PIL import Image\n","from sklearn.metrics import confusion_matrix\n","from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n","from torch.utils.data import Dataset\n","from torchvision.io import read_image, ImageReadMode\n","from torchvision.transforms.functional import resize as im_resize\n","from tqdm import tqdm\n","from typing import Optional, Tuple\n","from zipfile import ZipFile"],"metadata":{"id":"Iz1sZcx29iqW","executionInfo":{"status":"ok","timestamp":1672392661466,"user_tz":-120,"elapsed":5160,"user":{"displayName":"Hussein Fawzy","userId":"10172350209806464456"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"02a9acae-9511-483f-a898-b73e97fbe75c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["#login to weights and biases\n","#weights and biases service is used to track experiment results\n","USE_WANDB = False\n","\n","if USE_WANDB: wandb.login()"],"metadata":{"id":"tu1nf74xZ-Xz","colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"status":"ok","timestamp":1672392676437,"user_tz":-120,"elapsed":14987,"user":{"displayName":"Hussein Fawzy","userId":"10172350209806464456"}},"outputId":"d2f167e6-c6ce-4a59-d349-5591bd057437"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "]},{"name":"stdout","output_type":"stream","text":["··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}]},{"cell_type":"code","source":["#set project directories\n","ROOT_PATH = '/content/data/'\n","DATASETS_DIR = 'datasets'\n","EMBEDDINGS_DIR = 'embeddings'\n","MODELS_DIR = 'models'\n","BASE_MODEL_DIR = os.path.join(MODELS_DIR, 'base')\n","PREDICTIONS_DIR = 'predictions'\n","\n","#create required directories\n","os.makedirs(os.path.join(ROOT_PATH, DATASETS_DIR), exist_ok = True)\n","os.makedirs(os.path.join(ROOT_PATH, EMBEDDINGS_DIR), exist_ok = True)\n","os.makedirs(os.path.join(ROOT_PATH, BASE_MODEL_DIR), exist_ok = True) #will also create models dir\n","os.makedirs(os.path.join(ROOT_PATH, PREDICTIONS_DIR), exist_ok = True)"],"metadata":{"id":"6V-4odh4C_9J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#prepare processing device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","torch.cuda.set_device(0)\n","\n","dist_url = 'tcp://127.0.0.1:6769'\n","dist_url = dist_url[:-2] + str(os.getpid() % 100).zfill(2)\n","dist.init_process_group(backend = \"nccl\", init_method = dist_url, world_size = 1, rank = 0,)"],"metadata":{"id":"ZeP5LA8kOdqB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#copy required classes and functions from the original paper code\n","\n","#######################################################################################################################################\n","#decode_head.py\n","\n","#from ..builder import build_loss\n","#from ..losses import accuracy\n","#from mmcv.cnn import normal_init\n","#from mmcv.runner import auto_fp16, force_fp32\n","#from mmseg.core import build_pixel_sampler\n","#from mmseg.ops import resize\n","\n","\n","class BaseDecodeHead(nn.Module, metaclass=ABCMeta):\n","    \"\"\"Base class for BaseDecodeHead.\n","    Args:\n","        in_channels (int|Sequence[int]): Input channels.\n","        channels (int): Channels after modules, before conv_seg.\n","        num_classes (int): Number of classes.\n","        dropout_ratio (float): Ratio of dropout layer. Default: 0.1.\n","        conv_cfg (dict|None): Config of conv layers. Default: None.\n","        norm_cfg (dict|None): Config of norm layers. Default: None.\n","        act_cfg (dict): Config of activation layers.\n","            Default: dict(type='ReLU')\n","        in_index (int|Sequence[int]): Input feature index. Default: -1\n","        input_transform (str|None): Transformation type of input features.\n","            Options: 'resize_concat', 'multiple_select', None.\n","            'resize_concat': Multiple feature maps will be resize to the\n","                same size as first one and than concat together.\n","                Usually used in FCN head of HRNet.\n","            'multiple_select': Multiple feature maps will be bundle into\n","                a list and passed into decode head.\n","            None: Only one select feature map is allowed.\n","            Default: None.\n","        loss_decode (dict): Config of decode loss.\n","            Default: dict(type='CrossEntropyLoss').\n","        ignore_index (int | None): The label index to be ignored. When using\n","            masked BCE loss, ignore_index should be set to None. Default: 255\n","        sampler (dict|None): The config of segmentation map sampler.\n","            Default: None.\n","        align_corners (bool): align_corners argument of F.interpolate.\n","            Default: False.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 in_channels,\n","                 channels,\n","                 *,\n","                 num_classes,\n","                 dropout_ratio=0.1,\n","                 conv_cfg=None,\n","                 norm_cfg=None,\n","                 act_cfg=dict(type='ReLU'),\n","                 in_index=-1,\n","                 input_transform=None,\n","                 loss_decode=dict(\n","                     type='CrossEntropyLoss',\n","                     use_sigmoid=False,\n","                     loss_weight=1.0),\n","                 ignore_index=255,\n","                 sampler=None,\n","                 align_corners=False):\n","        super(BaseDecodeHead, self).__init__()\n","        self._init_inputs(in_channels, in_index, input_transform)\n","        self.channels = channels\n","        self.num_classes = num_classes\n","        self.dropout_ratio = dropout_ratio\n","        self.conv_cfg = conv_cfg\n","        self.norm_cfg = norm_cfg\n","        self.act_cfg = act_cfg\n","        self.in_index = in_index\n","        self.loss_decode = None # build_loss(loss_decode)\n","        self.ignore_index = ignore_index\n","        self.align_corners = align_corners\n","        # if sampler is not None:\n","        #     self.sampler = build_pixel_sampler(sampler, context=self)\n","        # else:\n","        #     self.sampler = None\n","        self.sampler = None\n","\n","        self.conv_seg = nn.Conv2d(channels, num_classes, kernel_size=1)\n","        if dropout_ratio > 0:\n","            self.dropout = nn.Dropout2d(dropout_ratio)\n","        else:\n","            self.dropout = None\n","        self.fp16_enabled = False\n","\n","    def extra_repr(self):\n","        pass\n","        # \"\"\"Extra repr.\"\"\"\n","        # s = f'input_transform={self.input_transform}, ' \\\n","        #     f'ignore_index={self.ignore_index}, ' \\\n","        #     f'align_corners={self.align_corners}'\n","        # return s\n","\n","    def _init_inputs(self, in_channels, in_index, input_transform):\n","        # \"\"\"Check and initialize input transforms.\n","        #\n","        # The in_channels, in_index and input_transform must match.\n","        # Specifically, when input_transform is None, only single feature map\n","        # will be selected. So in_channels and in_index must be of type int.\n","        # When input_transform\n","        #\n","        # Args:\n","        #     in_channels (int|Sequence[int]): Input channels.\n","        #     in_index (int|Sequence[int]): Input feature index.\n","        #     input_transform (str|None): Transformation type of input features.\n","        #         Options: 'resize_concat', 'multiple_select', None.\n","        #         'resize_concat': Multiple feature maps will be resize to the\n","        #             same size as first one and than concat together.\n","        #             Usually used in FCN head of HRNet.\n","        #         'multiple_select': Multiple feature maps will be bundle into\n","        #             a list and passed into decode head.\n","        #         None: Only one select feature map is allowed.\n","        # \"\"\"\n","        #\n","        if input_transform is not None:\n","            assert input_transform in ['resize_concat', 'multiple_select']\n","        self.input_transform = input_transform\n","        self.in_index = in_index\n","        if input_transform is not None:\n","            assert isinstance(in_channels, (list, tuple))\n","            assert isinstance(in_index, (list, tuple))\n","            assert len(in_channels) == len(in_index)\n","            if input_transform == 'resize_concat':\n","                self.in_channels = sum(in_channels)\n","            else:\n","                self.in_channels = in_channels\n","        else:\n","            assert isinstance(in_channels, int)\n","            assert isinstance(in_index, int)\n","            self.in_channels = in_channels\n","\n","    def init_weights(self):\n","        pass\n","        # \"\"\"Initialize weights of classification layer.\"\"\"\n","        # normal_init(self.conv_seg, mean=0, std=0.01)\n","\n","    def _transform_inputs(self, inputs):\n","        pass\n","        # \"\"\"Transform inputs for decoder.\n","        #\n","        # Args:\n","        #     inputs (list[Tensor]): List of multi-level img features.\n","        #\n","        # Returns:\n","        #     Tensor: The transformed inputs\n","        # \"\"\"\n","        #\n","        # if self.input_transform == 'resize_concat':\n","        #     inputs = [inputs[i] for i in self.in_index]\n","        #     upsampled_inputs = [\n","        #         resize(\n","        #             input=x,\n","        #             size=inputs[0].shape[2:],\n","        #             mode='bilinear',\n","        #             align_corners=self.align_corners) for x in inputs\n","        #     ]\n","        #     inputs = torch.cat(upsampled_inputs, dim=1)\n","        # elif self.input_transform == 'multiple_select':\n","        #     inputs = [inputs[i] for i in self.in_index]\n","        # else:\n","        #     inputs = inputs[self.in_index]\n","        #\n","        # return inputs\n","\n","    #@auto_fp16()\n","    def forward(self, inputs):\n","        \"\"\"Placeholder of forward function.\"\"\"\n","        pass\n","\n","    def forward_train(self, inputs, img_metas, gt_semantic_seg, train_cfg):\n","        pass\n","        # \"\"\"Forward function for training.\n","        # Args:\n","        #     inputs (list[Tensor]): List of multi-level img features.\n","        #     img_metas (list[dict]): List of image info dict where each dict\n","        #         has: 'img_shape', 'scale_factor', 'flip', and may also contain\n","        #         'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.\n","        #         For details on the values of these keys see\n","        #         `mmseg/datasets/pipelines/formatting.py:Collect`.\n","        #     gt_semantic_seg (Tensor): Semantic segmentation masks\n","        #         used if the architecture supports semantic segmentation task.\n","        #     train_cfg (dict): The training config.\n","        #\n","        # Returns:\n","        #     dict[str, Tensor]: a dictionary of loss components\n","        # \"\"\"\n","        # seg_logits = self.forward(inputs)\n","        # losses = self.losses(seg_logits, gt_semantic_seg)\n","        # return losses\n","\n","    def forward_test(self, inputs, img_metas, test_cfg):\n","        pass\n","        # \"\"\"Forward function for testing.\n","        #\n","        # Args:\n","        #     inputs (list[Tensor]): List of multi-level img features.\n","        #     img_metas (list[dict]): List of image info dict where each dict\n","        #         has: 'img_shape', 'scale_factor', 'flip', and may also contain\n","        #         'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.\n","        #         For details on the values of these keys see\n","        #         `mmseg/datasets/pipelines/formatting.py:Collect`.\n","        #     test_cfg (dict): The testing config.\n","        #\n","        # Returns:\n","        #     Tensor: Output segmentation map.\n","        # \"\"\"\n","        # output = self.forward(inputs)\n","        # output = self.emb2cls(output)\n","        # return output\n","\n","    def cls_seg(self, feat):\n","        \"\"\"Classify each pixel.\"\"\"\n","        if self.dropout is not None:\n","            feat = self.dropout(feat)\n","        output = self.conv_seg(feat)\n","        return output\n","\n","    def emb2cls(self, cls_score):\n","        pass\n","        # if hasattr(self.loss_decode, 'vec'):\n","        #     # normalize\n","        #     vec = self.loss_decode.vec.to(device=cls_score.device)\n","        #     if hasattr(self.loss_decode, 'norm'):\n","        #         cls_score = cls_score / cls_score.norm(dim=1, keepdim=True)\n","        #         vec = vec / vec.norm(dim=1, keepdim=True)\n","        #     if hasattr(self.loss_decode, 'logit_scale'):\n","        #         logit_scale = self.loss_decode.logit_scale\n","        #         cls_score = logit_scale * cls_score.permute(0, 2, 3, 1) @ vec.t()  # [N, H, W, num_cls]\n","        #     else:\n","        #         cls_score = cls_score.permute(0, 2, 3, 1) @ vec.t()  # [N, H, W, num_cls]\n","        #     cls_score = cls_score.permute(0, 3, 1, 2)  # [N, num_cls, H, W]\n","        #     return cls_score\n","        # else:\n","        #     raise NameError(\"No vec in loss_decode\")\n","\n","    #@force_fp32(apply_to=('seg_logit', ))\n","    def losses(self, seg_logit, seg_label):\n","        pass\n","        # \"\"\"Compute segmentation loss.\"\"\"\n","        # loss = dict()\n","        # seg_logit = resize(\n","        #     input=seg_logit,\n","        #     size=seg_label.shape[2:],\n","        #     mode='bilinear',\n","        #     align_corners=self.align_corners)\n","        # if self.sampler is not None:\n","        #     seg_weight = self.sampler.sample(seg_logit, seg_label)\n","        # else:\n","        #     seg_weight = None\n","        # seg_label = seg_label.squeeze(1)\n","        # loss['loss_seg'] = self.loss_decode(\n","        #     seg_logit,\n","        #     seg_label,\n","        #     weight=seg_weight,\n","        #     ignore_index=self.ignore_index)\n","        # print(seg_logit.shape, )\n","        # seg_logit = self.emb2cls(seg_logit)\n","        # loss['acc_seg'] = accuracy(seg_logit, seg_label)\n","        # return loss\n","\n","\n","#######################################################################################################################################\n","#segformer_head.py\n","\n","class MLP(nn.Module):\n","    \"\"\"\n","    Linear Embedding\n","    \"\"\"\n","    def __init__(self, input_dim=2048, embed_dim=768):\n","        super().__init__()\n","        self.proj = nn.Linear(input_dim, embed_dim)\n","\n","    def forward(self, x):\n","        x = x.flatten(2).transpose(1, 2)\n","        x = self.proj(x)\n","        return x\n","\n","\n","class SegFormerHead(BaseDecodeHead):\n","    \"\"\"\n","    SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers\n","    \"\"\"\n","    def __init__(self, feature_strides, **kwargs):\n","        super(SegFormerHead, self).__init__(input_transform='multiple_select', **kwargs)\n","        assert len(feature_strides) == len(self.in_channels)\n","        assert min(feature_strides) == feature_strides[0]\n","        self.feature_strides = feature_strides\n","\n","        c1_in_channels, c2_in_channels, c3_in_channels, c4_in_channels = self.in_channels\n","\n","        decoder_params = dict(embed_dim=768)#kwargs['decoder_params']\n","        embedding_dim = decoder_params['embed_dim']\n","\n","        self.linear_c4 = MLP(input_dim=c4_in_channels, embed_dim=embedding_dim)\n","        self.linear_c3 = MLP(input_dim=c3_in_channels, embed_dim=embedding_dim)\n","        self.linear_c2 = MLP(input_dim=c2_in_channels, embed_dim=embedding_dim)\n","        self.linear_c1 = MLP(input_dim=c1_in_channels, embed_dim=embedding_dim)\n","\n","        self.linear_fuse = ConvModule(\n","            in_channels=embedding_dim*4,\n","            out_channels=embedding_dim,\n","            kernel_size=1,\n","            norm_cfg=dict(type='SyncBN', requires_grad=True)\n","        )\n","\n","        self.linear_pred = nn.Conv2d(embedding_dim, self.num_classes, kernel_size=1)\n","\n","    def forward(self, inputs):\n","        x = inputs #self._transform_inputs(inputs)  # len=4, 1/4,1/8,1/16,1/32\n","        c1, c2, c3, c4 = x\n","\n","        ############## MLP decoder on C1-C4 ###########\n","        n, _, h, w = c4.shape\n","\n","        _c4 = self.linear_c4(c4).permute(0,2,1).reshape(n, -1, c4.shape[2], c4.shape[3])\n","        _c4 = resize(_c4, size=c1.size()[2:],mode='bilinear',align_corners=False)\n","\n","        \n","\n","        _c3 = self.linear_c3(c3).permute(0,2,1).reshape(n, -1, c3.shape[2], c3.shape[3])\n","        _c3 = resize(_c3, size=c1.size()[2:],mode='bilinear',align_corners=False)\n","\n","        _c2 = self.linear_c2(c2).permute(0,2,1).reshape(n, -1, c2.shape[2], c2.shape[3])\n","        _c2 = resize(_c2, size=c1.size()[2:],mode='bilinear',align_corners=False)\n","\n","        _c1 = self.linear_c1(c1).permute(0,2,1).reshape(n, -1, c1.shape[2], c1.shape[3])\n","\n","        _c = self.linear_fuse(torch.cat([_c4, _c3, _c2, _c1], dim=1))\n","\n","        x = self.dropout(_c)\n","\n","        x = self.linear_pred(x)\n","\n","        return x\n","\n","\n","#######################################################################################################################################\n","# transform_utils.py\n","\n","def get_imagenet_mean_std() -> Tuple[Tuple[float,float,float], Tuple[float,float,float]]:\n","    \"\"\" See use here in Pytorch ImageNet script: \n","        https://github.com/pytorch/examples/blob/master/imagenet/main.py#L197\n","        Returns:\n","        -   mean: Tuple[float,float,float], \n","        -   std: Tuple[float,float,float] = None\n","    \"\"\"\n","    value_scale = 255\n","    mean = [0.485, 0.456, 0.406]\n","    mean = [item * value_scale for item in mean]\n","    std = [0.229, 0.224, 0.225]\n","    std = [item * value_scale for item in std]\n","    return mean, std\n","\n","def normalize_img(  input: torch.Tensor, \n","                    mean: Tuple[float,float,float], \n","                    std: Optional[Tuple[float,float,float]] = None):\n","    \"\"\" Pass in by reference Torch tensor, and normalize its values.\n","        Args:\n","        -   input: Torch tensor of shape (3,M,N), must be in this order, and\n","                of type float (necessary).\n","        -   mean: mean values for each RGB channel\n","        -   std: standard deviation values for each RGB channel\n","        Returns:\n","        -   None\n","    \"\"\"\n","    if std is None:\n","        for t, m in zip(input, mean):\n","            t.sub_(m)\n","    else:\n","        for t, m, s in zip(input, mean, std):\n","            t.sub_(m).div_(s)\n","\n","def pad_to_crop_sz(\n","    image: np.ndarray,\n","    crop_h: int,\n","    crop_w: int,\n","    mean: Tuple[float,float,float]\n","    ) -> Tuple[np.ndarray,int,int]:\n","    ori_h, ori_w, _ = image.shape\n","    pad_h = max(crop_h - ori_h, 0)\n","    pad_w = max(crop_w - ori_w, 0)\n","    pad_h_half = int(pad_h / 2)\n","    pad_w_half = int(pad_w / 2)\n","    if pad_h > 0 or pad_w > 0:\n","        image = cv2.copyMakeBorder(\n","            src=image,\n","            top=pad_h_half,\n","            bottom=pad_h - pad_h_half,\n","            left=pad_w_half,\n","            right=pad_w - pad_w_half,\n","            borderType=cv2.BORDER_CONSTANT,\n","            value=mean)\n","    return image, pad_h_half, pad_w_half\n","\n","def resize_by_scaled_short_side(\n","    image: np.ndarray,\n","    base_size: int,\n","    scale: float) -> np.ndarray:\n","    \"\"\" Equivalent to ResizeShort(), but functional, instead of OOP paradigm, and w/ scale param.\n","\n","\tArgs:\n","\t    image: Numpy array of shape ()\n","\t    scale: scaling factor for image\n","\n","\tReturns:\n","\t    image_scaled:\n","    \"\"\"\n","    h, w, _ = image.shape\n","    short_size = round(scale * base_size)\n","    new_h = short_size\n","    new_w = short_size\n","    # Preserve the aspect ratio\n","    if h > w:\n","        new_h = round(short_size / float(w) * h)\n","    else:\n","        new_w = round(short_size / float(h) * w)\n","    image_scaled = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n","    return image_scaled\n","\n","\n","#######################################################################################################################################\n","#segformer.py\n","\n","class Mlp(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.dwconv = DWConv(hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, x, H, W):\n","        x = self.fc1(x)\n","        x = self.dwconv(x, H, W)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","\n","class Attention(nn.Module):\n","    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1):\n","        super().__init__()\n","        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n","\n","        self.dim = dim\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n","        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        self.sr_ratio = sr_ratio\n","        if sr_ratio > 1:\n","            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n","            self.norm = nn.LayerNorm(dim)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, x, H, W):\n","        B, N, C = x.shape\n","        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n","\n","        if self.sr_ratio > 1:\n","            x_ = x.permute(0, 2, 1).reshape(B, C, H, W)\n","            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)\n","            x_ = self.norm(x_)\n","            kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        else:\n","            kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        k, v = kv[0], kv[1]\n","\n","        attn = (q @ k.transpose(-2, -1)) * self.scale\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","\n","        return x\n","\n","\n","class Block(nn.Module):\n","    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n","                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1):\n","        super().__init__()\n","        self.norm1 = norm_layer(dim)\n","        self.attn = Attention(\n","            dim,\n","            num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            attn_drop=attn_drop, proj_drop=drop, sr_ratio=sr_ratio)\n","        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, x, H, W):\n","        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n","        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n","\n","        return x\n","\n","\n","class OverlapPatchEmbed(nn.Module):\n","    \"\"\" Image to Patch Embedding \"\"\"\n","\n","    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n","        super().__init__()\n","        img_size = to_2tuple(img_size)\n","        patch_size = to_2tuple(patch_size)\n","\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n","        self.num_patches = self.H * self.W\n","        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n","                              padding=(patch_size[0] // 2, patch_size[1] // 2))\n","        self.norm = nn.LayerNorm(embed_dim)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, x):\n","        x = self.proj(x)\n","        _, _, H, W = x.shape\n","        x = x.flatten(2).transpose(1, 2)\n","        x = self.norm(x)\n","\n","        return x, H, W\n","\n","\n","class MixVisionTransformer(nn.Module):\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dims=[64, 128, 256, 512],\n","                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n","                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n","                 depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1]):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.depths = depths\n","\n","        # patch_embed\n","        self.patch_embed1 = OverlapPatchEmbed(img_size=img_size, patch_size=7, stride=4, in_chans=in_chans,\n","                                              embed_dim=embed_dims[0]) # 1/4\n","        self.patch_embed2 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0],\n","                                              embed_dim=embed_dims[1]) # 1/8\n","        self.patch_embed3 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1],\n","                                              embed_dim=embed_dims[2]) # auxilary output\n","        self.patch_embed4 = OverlapPatchEmbed(img_size=img_size // 16, patch_size=3, stride=2, in_chans=embed_dims[2],\n","                                              embed_dim=embed_dims[3]) # 1/32\n","\n","        # transformer encoder\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n","        cur = 0\n","        self.block1 = nn.ModuleList([Block(\n","            dim=embed_dims[0], num_heads=num_heads[0], mlp_ratio=mlp_ratios[0], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n","            sr_ratio=sr_ratios[0])\n","            for i in range(depths[0])])\n","        self.norm1 = norm_layer(embed_dims[0])\n","\n","        cur += depths[0]\n","        self.block2 = nn.ModuleList([Block(\n","            dim=embed_dims[1], num_heads=num_heads[1], mlp_ratio=mlp_ratios[1], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n","            sr_ratio=sr_ratios[1])\n","            for i in range(depths[1])])\n","        self.norm2 = norm_layer(embed_dims[1])\n","\n","        cur += depths[1]\n","        self.block3 = nn.ModuleList([Block(\n","            dim=embed_dims[2], num_heads=num_heads[2], mlp_ratio=mlp_ratios[2], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n","            sr_ratio=sr_ratios[2])\n","            for i in range(depths[2])])\n","        self.norm3 = norm_layer(embed_dims[2])\n","\n","        cur += depths[2]\n","        self.block4 = nn.ModuleList([Block(\n","            dim=embed_dims[3], num_heads=num_heads[3], mlp_ratio=mlp_ratios[3], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n","            sr_ratio=sr_ratios[3])\n","            for i in range(depths[3])])\n","        self.norm4 = norm_layer(embed_dims[3])\n","\n","        # classification head\n","        # self.head = nn.Linear(embed_dims[3], num_classes) if num_classes > 0 else nn.Identity()\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def init_weights(self, pretrained=None):\n","        if isinstance(pretrained, str):\n","            logger = get_root_logger()\n","            load_checkpoint(self, pretrained, map_location='cpu', strict=False, logger=logger)\n","\n","    def reset_drop_path(self, drop_path_rate):\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(self.depths))]\n","        cur = 0\n","        for i in range(self.depths[0]):\n","            self.block1[i].drop_path.drop_prob = dpr[cur + i]\n","\n","        cur += self.depths[0]\n","        for i in range(self.depths[1]):\n","            self.block2[i].drop_path.drop_prob = dpr[cur + i]\n","\n","        cur += self.depths[1]\n","        for i in range(self.depths[2]):\n","            self.block3[i].drop_path.drop_prob = dpr[cur + i]\n","\n","        cur += self.depths[2]\n","        for i in range(self.depths[3]):\n","            self.block4[i].drop_path.drop_prob = dpr[cur + i]\n","\n","    def freeze_patch_emb(self):\n","        self.patch_embed1.requires_grad = False\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed1', 'pos_embed2', 'pos_embed3', 'pos_embed4', 'cls_token'}  # has pos_embed may be better\n","\n","    def get_classifier(self):\n","        return self.head\n","\n","    def reset_classifier(self, num_classes, global_pool=''):\n","        self.num_classes = num_classes\n","        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","    def forward_features(self, x):\n","        B = x.shape[0]\n","        outs = []\n","\n","        # stage 1\n","        x, H, W = self.patch_embed1(x)\n","        for i, blk in enumerate(self.block1):\n","            x = blk(x, H, W)\n","        x = self.norm1(x)\n","        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n","        outs.append(x)\n","\n","        # stage 2\n","        x, H, W = self.patch_embed2(x)\n","        for i, blk in enumerate(self.block2):\n","            x = blk(x, H, W)\n","        x = self.norm2(x)\n","        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n","        outs.append(x)\n","\n","        # stage 3\n","        x, H, W = self.patch_embed3(x)\n","        for i, blk in enumerate(self.block3):\n","            x = blk(x, H, W)\n","        x = self.norm3(x)\n","        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n","        outs.append(x)\n","\n","        # stage 4\n","        x, H, W = self.patch_embed4(x)\n","        for i, blk in enumerate(self.block4):\n","            x = blk(x, H, W)\n","        x = self.norm4(x)\n","        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n","        outs.append(x)\n","\n","        return outs\n","\n","    def forward(self, x):\n","        x = self.forward_features(x)\n","        # x = self.head(x)\n","\n","        return x\n","\n","\n","class DWConv(nn.Module):\n","    def __init__(self, dim=768):\n","        super(DWConv, self).__init__()\n","        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n","\n","    def forward(self, x, H, W):\n","        B, N, C = x.shape\n","        x = x.transpose(1, 2).view(B, C, H, W).contiguous()\n","        x = self.dwconv(x)\n","        x = x.flatten(2).transpose(1, 2)\n","\n","        return x\n","\n","\n","#@BACKBONES.register_module()\n","class mit_b5(MixVisionTransformer):\n","    def __init__(self, **kwargs):\n","        super(mit_b5, self).__init__(\n","            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 6, 40, 3], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)\n","\n","\n","#######################################################################################################################################\n","#color_seg.py\n","\n","def visual_segments(segments, rgb):\n","    seg = Image.fromarray(segments)\n","    rgb = Image.fromarray(rgb)\n","\n","    seg1 = seg.convert('RGBA')\n","    rgb1 = rgb.convert('RGBA')\n","\n","    vis_seg = Image.blend(rgb1, seg1, 0.8)\n","    return vis_seg\n","\n","def make_palette(num_classes=256):\n","    \"\"\"\n","    Inputs:\n","        num_classes: the number of classes\n","    Outputs:\n","        palette: the colormap as a k x 3 array of RGB colors\n","    \"\"\"\n","    palette = np.zeros((num_classes, 3), dtype=np.uint8)\n","    for k in range(0, num_classes):\n","        label = k\n","        i = 0\n","        while label:\n","            palette[k, 0] |= (((label >> 0) & 1) << (7 - i))\n","            palette[k, 1] |= (((label >> 1) & 1) << (7 - i))\n","            palette[k, 2] |= (((label >> 2) & 1) << (7 - i))\n","            label >>= 3\n","            i += 1\n","    idx1 = np.arange(0, num_classes, 2)[::-1]\n","    idx2 = np.arange(1, num_classes, 2)\n","    idx = np.concatenate([idx1[:, None], idx2[:, None]], axis=1).flatten()\n","    palette = palette[idx]\n","    palette[num_classes - 1, :] = [255, 255, 255]\n","    return palette\n","\n","def color_seg(seg, palette=None):\n","    PALETTE = make_palette(256)\n","    if palette == None:\n","        color_out = PALETTE[seg.reshape(-1)].reshape(seg.shape + (3,))\n","    else:\n","        color_out = palette[seg.reshape(-1)].reshape(seg.shape + (3,))\n","    return color_out\n","\n","\n","#######################################################################################################################################\n","#test.py\n","\n","def get_prediction(embs, gt_embs_list):\n","    prediction = []\n","    logits = []\n","    B, _, _, _ = embs.shape\n","    for b in range(B):\n","        score = embs[b,...]\n","        score = score.unsqueeze(0)\n","        emb = gt_embs_list\n","        emb = emb / emb.norm(dim=1, keepdim=True)\n","        score = score / score.norm(dim=1, keepdim=True)\n","        score = score.permute(0, 2, 3, 1) @ emb.t()\n","        # [N, H, W, num_cls] You maybe need to remove the .t() based on the shape of your saved .npy\n","        score = score.permute(0, 3, 1, 2)  # [N, num_cls, H, W]\n","        prediction.append(score.max(1)[1])\n","        logits.append(score)\n","    if len(prediction) == 1:\n","        prediction = prediction[0]\n","        logit = logits[0]\n","    else:\n","        prediction = torch.cat(prediction, dim=0)\n","        logit = torch.cat(logits, dim=0)\n","    return logit"],"metadata":{"id":"VOAWEPGclmnf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#the following are edited functions and classes from the original paper code\n","\n","class SegFormer(nn.Module):\n","  def __init__(self, num_classes, model_name = None):\n","    super(SegFormer, self).__init__()\n","\n","    self.encoder = mit_b5()\n","    self.head = SegFormerHead(num_classes = num_classes,\n","                              in_channels = [64, 128, 320, 512],\n","                              channels = 128,\n","                              in_index = [0, 1, 2, 3],\n","                              feature_strides = [4, 8, 16, 32],\n","                              #decoder_params = dict(embed_dim = 768),\n","                              dropout_ratio = 0.1,\n","                              norm_cfg = dict(type = 'SyncBN', requires_grad = True),\n","                              align_corners = False)\n","\n","    self.init_weights(model_name = model_name)\n","\n","  def init_weights(self, model_name = None) -> None:\n","    base_model_path = os.path.join(ROOT_PATH, BASE_MODEL_DIR, BASE_MODEL_FILENAME)\n","\n","    #load encoder\n","    self.encoder.eval()\n","\n","    encoder_checkpoint = torch.load(base_model_path, map_location = 'cpu')['state_dict']\n","    encoder_filter = {k[24:]: v for k, v in encoder_checkpoint.items() if 'module.segmodel.encoder' in k}\n","    self.encoder.load_state_dict(encoder_filter, strict = True)\n","    self.encoder.to(device)\n","\n","    #load head\n","    self.head.eval()\n","\n","    if model_name == None:\n","      head_model = torch.load(base_model_path, map_location = 'cpu')['state_dict']\n","      head_filter = {k[21:]: v for k, v in head_model.items() if 'module.segmodel.head' in k}\n","      self.head.load_state_dict(head_filter, strict=True)\n","    else:\n","      head_path = os.path.join(ROOT_PATH, MODELS_DIR, model_name + '.pt')\n","      head_model = torch.load(head_path)\n","      self.head.load_state_dict(head_model['head_state_dict'])\n","\n","    self.head.to(device)\n","  \n","  def set_train(self):\n","    self.encoder.train()\n","    self.head.train()\n","\n","  def set_eval(self):\n","    self.encoder.eval()\n","    self.head.eval()\n","\n","  def forward(self, x, h, w):\n","    out = self.head(x)\n","    high_out = F.interpolate(out, size=(h, w), mode = 'bilinear', align_corners = True)\n","\n","    return high_out, out, None\n","\n","\n","def single_scale_single_crop_cuda(model, image: np.ndarray, h: int, w: int, gt_embs_list) -> np.ndarray:\n","    ori_h, ori_w, _ = image.shape\n","    mean, std = get_imagenet_mean_std()\n","    crop_h = (np.ceil((ori_h - 1) / 32) * 32).astype(np.int32)\n","    crop_w = (np.ceil((ori_w - 1) / 32) * 32).astype(np.int32)\n","    \n","    #pad and normalize image\n","    image, pad_h_half, pad_w_half = pad_to_crop_sz(image, crop_h, crop_w, mean)\n","    image = torch.from_numpy(image.transpose((2, 0, 1))).float()\n","    normalize_img(image, mean, std)\n","    image = image.unsqueeze(0).cuda()\n","\n","    #obtain output\n","    with torch.no_grad():\n","      x = model.encoder(image)\n","\n","    emb, _, _ = model(x, h, w)\n","    logit = get_prediction(emb, gt_embs_list)\n","    \n","    logit_universal = F.softmax(logit * 100, dim = 1).squeeze()\n","\n","    # disregard predictions from padded portion of image\n","    prediction_crop = logit_universal[:, pad_h_half:pad_h_half + ori_h, pad_w_half:pad_w_half + ori_w]\n","\n","    prediction_crop = im_resize(prediction_crop, (h, w))\n","    prediction_crop = prediction_crop.permute(1, 2, 0)\n","\n","    return prediction_crop"],"metadata":{"id":"iuT8nd_sRSKD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#model functions\n","\n","def build_embeddings(labels, definitions, filename, force_rebuild = False):\n","  emb_path = os.path.join(ROOT_PATH, EMBEDDINGS_DIR, filename + '.e')\n","\n","  if force_rebuild == False and os.path.exists(emb_path):\n","    #load text embeddings\n","    text_embs = torch.load(emb_path)\n","    return text_embs\n","\n","  #load CLIP model\n","  clip_model, _ = clip.load('ViT-B/32', device)\n","\n","  #encode definitions\n","  definition_list = [definitions[labels[i]] for i in labels]\n","  text_inputs = torch.cat([clip.tokenize(definition) for definition in definition_list]).to(device)\n","  text_embs = clip_model.encode_text(text_inputs).float()\n","\n","  torch.save(text_embs, emb_path)\n","  text_embs = torch.load(emb_path)\n","\n","  return text_embs\n","\n","def fine_tune(model, data, optimizer, text_embs, epochs = 1):\n","  #sets the model in training mode\n","  model.set_train()\n","\n","  #freeze encoder parameters\n","  for param in model.encoder.parameters():\n","      param.requires_grad = False\n","\n","  #set loss criterion\n","  loss_criterion = nn.CrossEntropyLoss()\n","\n","  #train\n","  losses = []\n","\n","  for epoch in range(epochs):\n","    epoch_losses = []\n","\n","    for (input, target) in tqdm(data):\n","      #predict\n","      h, w, _ = input.shape\n","      input = resize_by_scaled_short_side(input, 720, 1)\n","      pred = single_scale_single_crop_cuda(model, input, h, w, gt_embs_list = text_embs)\n","\n","      #clean memory\n","      del input\n","      gc.collect()\n","      torch.cuda.empty_cache()\n","\n","      #compute loss\n","      target = target.flatten().cuda()\n","      pred = pred.reshape((-1, 12))\n","      optimizer.zero_grad()\n","      loss = loss_criterion(pred, target)\n","\n","      #clean memory\n","      del pred, target\n","      gc.collect()\n","      torch.cuda.empty_cache()\n","\n","      #back-propagate\n","      loss.backward()\n","      optimizer.step()\n","\n","      loss = loss.cpu().detach().numpy().item()\n","      epoch_losses.append(loss)\n","\n","    epoch_loss = sum(epoch_losses) / len(epoch_losses)\n","    losses += epoch_losses\n","\n","    if USE_WANDB: wandb.log({\"loss\": epoch_loss})\n","    print('Epoch {}: loss = {}'.format(epoch + 1, epoch_loss))\n","    print()\n","\n","  plt.plot(losses)\n","  plt.ylabel(\"Loss (CrossEntropy)\")\n","\n","def calculate_iou(conf_mat, multiplier = 1.0):\n","  cm = conf_mat.copy()\n","\n","  np.fill_diagonal(cm, np.diag(cm) * multiplier)\n","\n","  inter = np.diag(cm)\n","  gt_set = cm.sum(axis = 1)\n","  pred_set = cm.sum(axis = 0)\n","\n","  union_set =  gt_set + pred_set - inter\n","  iou = inter.astype(float) / union_set\n","  mean_iou = np.nanmean(iou)\n","\n","  return mean_iou\n","\n","def evaluate_performance(model, data, labels, text_embs):\n","  text_embs.requires_grad = False\n","\n","  #sets the model in evaluation mode (not trainable)\n","  model.set_eval()\n","  \n","  np.seterr(invalid = 'ignore')\n","  iou = 0\n","\n","  for (input, target) in tqdm(data):\n","    #predict\n","    h, w, _ = input.shape\n","    input = resize_by_scaled_short_side(input, 720, 1)\n","    pred = single_scale_single_crop_cuda(model, input, h, w, gt_embs_list = text_embs)\n","\n","    #clean memory\n","    del input\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    #calculate intersection-over-union\n","    pred = pred.argmax(axis = -1).squeeze().flatten()\n","    target = target.flatten().cuda()\n","    conf_mat = confusion_matrix(y_pred = pred.cpu(), y_true = target.cpu(), labels = list(labels.keys()))\n","    iou += calculate_iou(conf_mat)\n","\n","    #clean memory\n","    del pred, target\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","  #calculate mean iou\n","  mean_iou = iou / len(data)\n","  np.seterr(invalid = 'warn')\n","\n","  return mean_iou\n","\n","def save_predictions(model, data, text_embs, sub_dir, n = 10):\n","  sub_path = os.path.join(ROOT_PATH, PREDICTIONS_DIR, sub_dir)\n","  os.makedirs(sub_path, exist_ok = True)\n","\n","  text_embs.requires_grad = False\n","\n","  #sets the model in evaluation mode (not trainable)\n","  model.set_eval()\n","\n","  for i, (input, target) in enumerate(data):\n","    if i == n: break\n","\n","    #predict\n","    h, w, _ = input.shape\n","    input_resized = resize_by_scaled_short_side(input, 720, 1)\n","    pred = single_scale_single_crop_cuda(model, input_resized, h, w, gt_embs_list = text_embs)\n","\n","    #clean memory\n","    del input_resized\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    #overlay colors\n","    pred = pred.argmax(axis = -1).squeeze()\n","    pred = pred.cpu().numpy()\n","\n","    pred_color = color_seg(pred)\n","    vis_seg = visual_segments(pred_color, input)\n","\n","    #save the image with _p (for prediction) and its original number identifier\n","    save_path = os.path.join(sub_path, \"cmp_p\" + str(i + 1).zfill(4) + '.png')\n","    vis_seg.save(save_path)"],"metadata":{"id":"UaTrloNNIKP1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MAX_MEMORY = 700000\n","\n","#cmp dataset iterator class\n","class CMPDataset(Dataset):\n","  def __init__(self, dataset_path):\n","    super(CMPDataset, self).__init__()\n","\n","    self.dataset_path = dataset_path\n","    self.pairs = []\n","\n","    filenames = os.listdir(self.dataset_path)\n","    images_count = int(len(filenames) / 3)    #each sample is represented with 3 files\n","    prefix = filenames[0][:5]                 #extract prefix from the first filename\n","\n","    for i in range(1, images_count + 1):\n","      input_path = os.path.join(self.dataset_path, prefix + str(i).zfill(4) + '.jpg')\n","      target_path = os.path.join(self.dataset_path, prefix + str(i).zfill(4) + '.png')\n","\n","      input = cv2.imread(input_path, cv2.IMREAD_UNCHANGED)[:, :, ::-1]      #read and reverse RGB channels to BGR (to match the original code)\n","      target = read_image(target_path, mode = ImageReadMode.UNCHANGED) - 1  #load pixel labels starting from 0\n","\n","      #limit max image memory to avoid OutOfMemoryError\n","      #only the input is checked as it consumes more memory than the target\n","      if input.shape[0] * input.shape[1] > MAX_MEMORY: continue\n","\n","      self.pairs.append((input, target))\n","\n","  def __len__(self):\n","    return len(self.pairs)\n","\n","  def __getitem__(self, idx):\n","    return self.pairs[idx]\n","\n","\n","#dataset download function\n","def download_extract_dataset(dataset_name):\n","  dataset_filename = dataset_name + \".zip\"\n","  dataset_url = \"https://cmp.felk.cvut.cz/~tylecr1/facade/\" + dataset_filename\n","\n","  #skip if already downloaded\n","  dataset_filepath = os.path.join(ROOT_PATH, DATASETS_DIR, dataset_filename)\n","  if not os.path.exists(dataset_filepath):\n","    p = Path(dataset_filepath)\n","    response = requests.get(dataset_url)\n","    p.write_bytes(response.content)\n","\n","  #skip if already extracted\n","  dataset_extract_path = os.path.join(ROOT_PATH, DATASETS_DIR, dataset_name)\n","  if not os.path.exists(dataset_extract_path):\n","    with ZipFile(dataset_filepath, 'r') as z_object: z_object.extractall(path = dataset_extract_path)"],"metadata":{"id":"OposWBn6Pt53"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#download and extract base and extended datasets\n","BASE_DATASET_NAME = \"CMP_facade_DB_base\"\n","EXTENDED_DATASET_NAME = \"CMP_facade_DB_extended\"\n","\n","download_extract_dataset(BASE_DATASET_NAME)\n","download_extract_dataset(EXTENDED_DATASET_NAME)\n","\n","#set path where the datasets are extracted\n","BASE_DATASET_PATH = os.path.join(ROOT_PATH, DATASETS_DIR, BASE_DATASET_NAME, 'base')\n","EXTENDED_DATASET_PATH = os.path.join(ROOT_PATH, DATASETS_DIR, EXTENDED_DATASET_NAME, 'extended')\n","\n","#download base model\n","BASE_MODEL_FILENAME = \"segformer_7data.pth\"\n","BASE_MODEL_URL = \"https://cloudstor.aarnet.edu.au/plus/s/AtYYaVSVVAlEwve/download\"\n","\n","model_filepath = os.path.join(ROOT_PATH, BASE_MODEL_DIR, BASE_MODEL_FILENAME)\n","if not os.path.exists(model_filepath):\n","  p = Path(model_filepath)\n","  response = requests.get(BASE_MODEL_URL)\n","  p.write_bytes(response.content)"],"metadata":{"id":"hRpEnDHwPts3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#labels and their definitions \n","\n","labels = {\n","    1: \"background\",\n","    2: \"facade\",\n","    3: \"window\",\n","    4: \"door\",\n","    5: \"cornice\",\n","    6: \"sill\",\n","    7: \"balcony\",\n","    8: \"blind\",\n","    9: \"deco\",\n","    10: \"molding\",\n","    11: \"pillar\",\n","    12: \"shop\"\n","}\n","\n","#word and sentence definitions are introduced to check the effect of sentence labels on the model\n","#sentence definitions are obtained from https://www.merriam-webster.com/\n","definitions = {\n","  \"word\":     {v: v for _, v in labels.items()},\n","               \n","  \"sentence\": {\"background\": \"This is an image of background. A background is the part of a scene representing what lies behind objects in the foreground\",\n","              \"facade\": \"This is an image of facade. A facade is the front of a building\",\n","              \"window\": \"This is an image of window. A window is an opening especially in the wall of a building for admission of light and air that is usually closed by casements or sashes containing transparent material (such as glass) and capable of being opened and shut\",\n","              \"door\": \"This is an image of door. A door is a usually swinging or sliding barrier by which an entry is closed and opened\",\n","              \"cornice\": \"This is an image of cornice. A cornice is the molded and projecting horizontal member that crowns an architectural composition\",\n","              \"sill\": \"This is an image of window sill. A sill is a horizontal piece (such as a timber) that forms the lowest member or one of the lowest members of a framework or supporting structure\",\n","              \"balcony\": \"This is an image of balcony. A balcony is a platform that projects from the wall of a building and is enclosed by a parapet or railing\",\n","              \"blind\": \"This is an image of blind. A blind is something to hinder sight or keep out light\",\n","              \"deco\": \"This is an image of decoration. A decoration is something that adorns, enriches, or beautifies\",\n","              \"molding\": \"This is an image of molding. A molding is a decorative recessed or relieved surface\",\n","              \"pillar\": \"This is an image of pillar. A pillar is a firm upright support for a superstructure\",\n","              \"shop\": \"This is an image of shop. A shop is a building or room stocked with merchandise for sale\"}\n","}"],"metadata":{"id":"TfVMfXFdI0tN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#calculate and save new embeddings using definitions\n","DEFINITIONS_TYPE = \"word\" #\"word\" or \"sentence\"\n","\n","emb_path = os.path.join(ROOT_PATH, EMBEDDINGS_DIR, DEFINITIONS_TYPE + '.e')\n","text_embs = build_embeddings(labels, definitions[DEFINITIONS_TYPE], DEFINITIONS_TYPE, force_rebuild = False)"],"metadata":{"id":"OtIe6s1bfSoc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672392741765,"user_tz":-120,"elapsed":16703,"user":{"displayName":"Hussein Fawzy","userId":"10172350209806464456"}},"outputId":"bd6ba9bb-de1a-48ae-ce03-808f66c77b03"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 338M/338M [00:05<00:00, 61.6MiB/s]\n"]}]},{"cell_type":"code","source":["#load image datasets\n","train_data = CMPDataset(BASE_DATASET_PATH)\n","test_data = CMPDataset(EXTENDED_DATASET_PATH)\n","\n","#load saved model\n","model = SegFormer(512, model_name = None) #set model_name to an already fine-tuned model to start from there\n","\n","#set hyper-parameters\n","OPTIMIZER_NAME = \"adam\" #\"adam\" or \"sgd\"\n","LR = 0.00006\n","EPOCHS = 50\n","\n","if OPTIMIZER_NAME == \"adam\":\n","  optimizer = torch.optim.Adam(model.head.parameters(), lr = LR)\n","elif OPTIMIZER_NAME == \"sgd\":\n","  optimizer = torch.optim.SGD(model.head.parameters(), lr = LR, momentum = 0.9)\n","\n","if USE_WANDB: wandb.init(\n","    # set the wandb project where this run will be logged\n","    project = \"ssiw_transfer_learning_cmp\",\n","    \n","    # track hyperparameters and run metadata\n","    config = {\n","      \"embeddings\": DEFINITIONS_TYPE,\n","      \"dataset\": \"base\",\n","      \"optimizer\": OPTIMIZER_NAME,\n","      \"learning_rate\": LR,\n","      \"learning_rate_scheduler\": \"constant_lr\",\n","      \"epochs\": EPOCHS\n","    }\n",")\n","\n","#fine tune the model\n","fine_tune(model, train_data, optimizer, text_embs, epochs = EPOCHS)\n","\n","#save new checkpoint\n","MODEL_NAME = 'ssiw_fine_tuned_{}_{}_{}'.format(DEFINITIONS_TYPE, OPTIMIZER_NAME, EPOCHS)\n","\n","save_path = os.path.join(ROOT_PATH, MODELS_DIR , MODEL_NAME + '.pt')\n","torch.save({'head_state_dict': model.head.state_dict()}, save_path)"],"metadata":{"id":"73v6JqXzAcDw","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1672402030537,"user_tz":-120,"elapsed":9288774,"user":{"displayName":"Hussein Fawzy","userId":"10172350209806464456"}},"outputId":"4c44c6c8-2d54-4282-ad8a-96ac081d403c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhussein-fawzy\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.13.7"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20221230_093232-elcokbqf</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/hussein-fawzy/ssiw_transfer_learning_cmp/runs/elcokbqf\" target=\"_blank\">icy-lion-10</a></strong> to <a href=\"https://wandb.ai/hussein-fawzy/ssiw_transfer_learning_cmp\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:08<00:00,  1.09s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: loss = 2.2222030996587234\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:06<00:00,  1.08s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: loss = 2.0973763114455117\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3: loss = 2.0698123598374383\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4: loss = 2.048158101263763\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:04<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5: loss = 2.03646149800692\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6: loss = 2.0284500955846267\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7: loss = 2.0214234445825476\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8: loss = 2.0153539511509715\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9: loss = 2.011673494570517\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:04<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10: loss = 2.0081117394342587\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11: loss = 2.003615774860272\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12: loss = 2.001730638432365\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13: loss = 1.9989040318252034\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:04<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14: loss = 1.9961765591119756\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:04<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15: loss = 1.9937165624144448\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:04<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16: loss = 1.991342617597194\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17: loss = 1.9883316148912287\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:04<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18: loss = 1.986637211948461\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19: loss = 1.9853319033032897\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20: loss = 1.9826472315485077\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:06<00:00,  1.08s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 21: loss = 1.9819437210270434\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 22: loss = 1.9792042171334945\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 23: loss = 1.979288027465688\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:06<00:00,  1.08s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 24: loss = 1.9782178960094563\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:09<00:00,  1.09s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 25: loss = 1.9771707836603154\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:06<00:00,  1.08s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 26: loss = 1.9735470424497747\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 27: loss = 1.9729590739817977\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 28: loss = 1.9723592748531718\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 29: loss = 1.9699563952539698\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:04<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 30: loss = 1.9699560514075218\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 31: loss = 1.969169800681186\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 32: loss = 1.967869556019072\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:04<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 33: loss = 1.9663743917652636\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:04<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 34: loss = 1.9656292558405442\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 35: loss = 1.9648469652054628\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 36: loss = 1.9645896171558799\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:04<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 37: loss = 1.9633263039451114\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 38: loss = 1.9628120095743609\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 39: loss = 1.9606768375187251\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:04<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 40: loss = 1.9604542165822376\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:04<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 41: loss = 1.9606212122591933\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:04<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 42: loss = 1.959452275595913\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 43: loss = 1.9589433980125912\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:04<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 44: loss = 1.9581640517780547\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 45: loss = 1.9572317372856802\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 46: loss = 1.955905358915384\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 47: loss = 1.9560296397677737\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 48: loss = 1.9534311301446374\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 49: loss = 1.9520844324475768\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 173/173 [03:05<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 50: loss = 1.950383275919567\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["#evaluate model performance\n","zero_shot_model = SegFormer(512)\n","\n","zero_shot_miou = evaluate_performance(zero_shot_model, test_data, labels, text_embs)\n","fine_tuned_miou = evaluate_performance(model, test_data, labels, text_embs)\n","\n","if USE_WANDB: \n","  wandb.log({\"zero-shot mIoU\": zero_shot_miou})\n","  wandb.log({\"fine-tuned mIoU\": fine_tuned_miou})\n","\n","print(\"\\nZero-shot mIoU:\", zero_shot_miou)\n","print(\"Fine-tuned mIoU:\", fine_tuned_miou)"],"metadata":{"id":"5f4ptfvgyp89","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672402227658,"user_tz":-120,"elapsed":197124,"user":{"displayName":"Hussein Fawzy","userId":"10172350209806464456"}},"outputId":"11174c7a-a234-4d20-c53a-00e8da12796b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 113/113 [01:37<00:00,  1.16it/s]\n","100%|██████████| 113/113 [01:36<00:00,  1.17it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Zero-shot mIoU: 0.13495909868473374\n","Fine-tuned mIoU: 0.241632248041045\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["#finish the wandb run, necessary in notebooks\n","if USE_WANDB: wandb.finish()"],"metadata":{"id":"W_hiOeNsew39","executionInfo":{"status":"ok","timestamp":1672402233156,"user_tz":-120,"elapsed":5506,"user":{"displayName":"Hussein Fawzy","userId":"10172350209806464456"}},"colab":{"base_uri":"https://localhost:8080/","height":377,"referenced_widgets":["3a39caac46f94a91b820594aecdfa6cc","1c9319480b9b43aa90ffa512014ea62d","cfbf4bf823664a4f902f2cea0cc616f6","f9247d172ae74aae9300f17abab71bc9","2a1c5db9b8a748c785fae5f9613b48ad","411fdaa25a1e4342ac66220389a46058","3cb8b83391f34cc5b7b3a462c770a86a","bcfa414ffd64410f93f38d95b3dcb299"]},"outputId":"25fe20c2-e704-4358-fd46-17bdfcc6fbe9"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a39caac46f94a91b820594aecdfa6cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>fine-tuned mIoU</td><td>▁</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>zero-shot mIoU</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>fine-tuned mIoU</td><td>0.24163</td></tr><tr><td>loss</td><td>1.95038</td></tr><tr><td>zero-shot mIoU</td><td>0.13496</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Synced <strong style=\"color:#cdcd00\">icy-lion-10</strong>: <a href=\"https://wandb.ai/hussein-fawzy/ssiw_transfer_learning_cmp/runs/elcokbqf\" target=\"_blank\">https://wandb.ai/hussein-fawzy/ssiw_transfer_learning_cmp/runs/elcokbqf</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20221230_093232-elcokbqf/logs</code>"]},"metadata":{}}]},{"cell_type":"code","source":["#save prediction images. helpful to visually analyze the results\n","zero_shot_predictions_path = os.path.join(ROOT_PATH, PREDICTIONS_DIR, \"zero_shot\")\n","\n","if os.path.exists(zero_shot_predictions_path) == False: #there is no need to run zero-shot prediction if they are already available\n","  save_predictions(zero_shot_model, test_data, text_embs, \"zero_shot\")\n","  \n","save_predictions(model, test_data, text_embs, MODEL_NAME)"],"metadata":{"id":"Jb5M27UkTxrL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Results"],"metadata":{"id":"0bZxTmKXKsM2"}},{"cell_type":"markdown","source":["Please check this **[link](https://api.wandb.ai/report/hussein-fawzy/lwkks95u)** for the report on the results"],"metadata":{"id":"2M3YiiabK1Z3"}},{"cell_type":"markdown","source":["# Future Work"],"metadata":{"id":"s9gdfJgUci3X"}},{"cell_type":"markdown","source":["In order to improve the performance and usability of the model, the following points should be taken into consideration:\n","*   Fine-tune the model on two steps;one, disable encoder training and only train the head (already done) on the new dataset, and two, enable encoder training and train the whole network on the new dataset with a lower learning rate\n","*   Use a validation set when fine-tuning the model\n","*   Try fine-tuning with a learning-rate decay model (ex. step or exponential)\n","*   Annotate output (predicted) images with labels"],"metadata":{"id":"LJrOirO8ck8H"}}]}